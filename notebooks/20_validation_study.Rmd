# Validation with real data
```{r, include=FALSE}
res_validation = new.env()
```

We use datasets which estimate the immune cell proportions using flow cytometry as an additional validation
for our simulation benchmark.  We use the following three validation datasets (see section \@ref(input-data)):
```{r, cache=TRUE}
datasets = list(
  racle=racle,
  hoek=hoek,
  schelker_ovarian=schelker_ovarian
)
```

We use the following cell types, which are available in (some of) the datasets
```{r}
use_cell_types = c("T cell", "T cell CD8+", "T cell CD4+",
                   "Monocyte", "B cell",
                   "Dendritic cell", "NK cell")
```

```{r, echo=FALSE, cache=TRUE, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
# run deconvolution and include TIMER results

## process a deconvolution result
process_result = function(result, method, dataset_name) {
  result %>%
    map_result_to_celltypes(use_cell_types, method) %>%
    as_tibble(rownames="cell_type") %>%
    na.omit() %>%
    gather(sample, estimate, -cell_type) %>%
    mutate(method=method, dataset=dataset_name)
}

timer_cancer_type = list(racle="SKCM", # actually PBMC
                         hoek="SKCM",
                         schelker_ovarian="OV")


# Run the deconvolution...
all_results = foreach(dataset=datasets, dataset_name=names(datasets), .combine=bind_rows) %:%
  foreach(method = config$deconvolution_methods, .combine=bind_rows) %do% {
    tumor = (dataset_name != "hoek") # hoek is PBMC, all others are tumor
    timer_indications = rep(timer_cancer_type[[dataset_name]], ncol(dataset$expr_mat))
    deconvolute(dataset$expr_mat, method, indications=timer_indications, tumor=tumor) %>%
      process_result(method, dataset_name)
}

all_refs = foreach(dataset=datasets, dataset_name=names(datasets), .combine=bind_rows) %do% {
  dataset$ref %>%
    select(sample, cell_type, true_fraction) %>%
    spread(sample, true_fraction) %>%
    map_result_to_celltypes(use_cell_types) %>%
    as_tibble(rownames="cell_type") %>%
    gather(sample, true_fraction, -cell_type) %>%
    mutate(dataset=dataset_name) %>%
    na.omit()
}
```

Here, we combine the predictions with the 'gold standard' reference data.
```{r, cache=TRUE}
all_results_ref = inner_join(all_results, all_refs,
                             by = c("sample" = "sample",
                                    "dataset" = "dataset",
                                    "cell_type" = "cell_type"))

res_validation$all_results = all_results_ref
```

The scores of the methods have different properties and not all of them are directly comparable.
We distinguish between three types of comparisons:

* absolute scores: allow to compare within *and* between samples.
* scores relative to the total immune cell content: allow to compare within a sample
* scores in arbitrary units, that only allow to compare between samples.

Here, we assign the methods to their respective group:
```{r}
abs_methods = c("cibersort_abs", "quantiseq", "epic")
within_methods = c("cibersort", "cibersort_abs", "quantiseq", "epic")
between_methods = c("cibersort_abs", "quantiseq", "epic", "timer", "xcell", "mcp_counter")
```

## Between- and within-sample comparisons
Only works for methods providing an absolute score (`r paste(abs_methods, collapse=", ")`).
All other methods are included for reference.

```{r, fig.width=10, fig.height=16, echo=FALSE, fig.cap="Comparison of absolute predictions for three validation dataset. "}
all_results_ref %>%
  mutate(dataset2 = dataset) %>%
  bind_rows(all_results_ref %>% mutate(dataset2 = "all")) %>%
  filter(!(cell_type == "T cell" & dataset != "hoek")) %>%
  ggplot(aes(x=true_fraction, y=estimate)) +
    geom_point(aes(color=cell_type, shape=dataset)) +
    scale_color_manual(values=color_scales$validation) +
    facet_grid(method~dataset2, scales = "free_y") +
    theme_bw() +
    theme(legend.position = "top",
          strip.text.x = element_text(size=9)) +
    background_grid(major="xy") +
    stat_cor()
```

```{r mixing_abs_val, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
slope = all_results_ref %>%
  filter(method %in% abs_methods) %>%
  group_by(cell_type) %>%
  mutate(n=n_distinct(sample)) %>%
  group_by(cell_type, method, n) %>%
  do(get_slope(.)) %>%
  ungroup()

cell_type_method_mat = crossing(abs_methods, use_cell_types)
colnames(cell_type_method_mat) = c("method", "cell_type")
res_validation$slope = slope
```

Next, we assess the absolute deviation of the methods. Like in the previous
section, we use two measures:

 * The slope of a linear model fitted to the data.
 * The root mean square error.

```{r, fig.width=9, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Comparison of absolute methods. Values indicate the slope of a linear model fitted to predictions vs. true fractions. Values < 1 indicate an under-prediction of the cell type, values > 1 an over-prediction respectively. The error bars have been computed using `confint` on the result of `lm`. "}
slope %>%
  right_join(cell_type_method_mat) %>%
  # na -> 0 workaround, otherwise text label does not show
  ggplot(aes(x=cell_type, y=ifelse(is.na(slope), 1, slope))) +
    geom_crossbar(aes(color=method, ymin=slope, ymax=slope), stat="identity") +
    geom_hline(yintercept=1, col="grey") +
    geom_text(aes(label=ifelse(is.na(slope), "n/a", NA)), angle=0) +
    geom_errorbar(aes(ymin=ci_lower, ymax=ci_upper), width=.2) +
    facet_wrap(~method, nrow=1, labeller = label_wrap_gen()) +
    theme(axis.text.x=element_text(angle = 90, vjust = 0.5, hjust=1),
          legend.position="top",
          strip.text.x = element_text(size=9)) +
    scale_fill_manual(values=color_scales$methods, na.value="grey") +
    scale_alpha_manual(values=c("yes"=.3, "no"=1.)) +
    coord_flip(y=c(-5, 10)) +
    ylab("slope of linear fit")
```
```{r abs-rmse-val, fig.width=9, fig.height=4, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Comparison of absolute methods. The values show the RMSE"}
rmse = all_results_ref %>%
  filter(method %in% abs_methods) %>%
  mutate(square_error = (estimate-true_fraction)^2) %>%
  group_by(method, cell_type) %>%
  summarise(rmse = sqrt(mean(square_error)))

rmse %>%
 right_join(cell_type_method_mat) %>%
 ggplot(aes(x=cell_type, y=ifelse(is.na(rmse), 0, rmse))) +
    geom_bar(aes(fill=method), stat="identity") +
    geom_text(aes(label=ifelse(is.na(rmse), "n/a", NA)), angle=90, y=0.05) +
    facet_wrap(~method, nrow=1, labeller = label_wrap_gen()) +
    theme(axis.text.x=element_text(angle = 90, vjust = 0.5, hjust=1),
          legend.position="top",
          strip.text.x = element_text(size=9)) +
    scale_fill_manual(values=color_scales$methods, na.value="grey") +
    scale_alpha_manual(values=c("yes"=.3, "no"=1.)) +
    coord_flip() +
    geom_hline(yintercept=0, col="grey") +
    ylab("RMSE")

res_validation$rmse = rmse
```

## Within-sample comparison
only works with methods that provide an absolute score, or a score that is relative to total immune
cell content (`r paste(within_methods, collapse=", ")`).

```{r, fig.width=8, fig.height=12, echo=FALSE, fig.cap="Comparison of predictions within each individual sample"}
all_results_ref %>%
  filter(method %in% within_methods) %>%
  filter(!(cell_type == "T cell" & dataset != "hoek")) %>%
  ggplot(aes(x=true_fraction, y=estimate)) +
    geom_point(aes(color=cell_type, shape=dataset)) +
    scale_color_manual(values=color_scales$validation) +
    facet_grid(sample~method, scales = "free_y") +
    theme_bw() +
    theme(legend.position = "right",
          strip.text.x = element_text(size=9),
          strip.text.y = element_text(size=7)) +
    background_grid(major="xy") +
    stat_cor(size=3)
```

Compute the average over all samples:
```{r, fig.width=8, fig.height=3, echo=FALSE, fig.cap="Correlations of within-sample comparisons. The last column shows the mean over all samples. "}
sample_correlations = all_results_ref %>%
  filter(method %in% within_methods) %>%
  filter(!(cell_type == "T cell" & dataset != "hoek")) %>%
  group_by(dataset, method, sample) %>%
  do(make_cor(.$true_fraction, .$estimate))

average = sample_correlations %>%
  group_by(method) %>%
  summarise(pearson = mean(pearson)) %>%
  mutate(sample = "mean") %>%
  mutate(dataset = "mean")

sample_correlations %>%
  bind_rows(average) %>%
  mutate(pearson_text = if_else(pearson < 0, "< 0", as.character(round(pearson, 2))),
         pearson = if_else(pearson < 0, 0, pearson)) %>%
  ggplot(aes(x=sample, y=method)) +
    geom_tile(aes(fill=pearson)) +
    geom_text(aes(label=pearson_text), size=3) +
    scale_fill_distiller(type="div", palette = "RdYlGn", direction=1, values=c(0,1))  +
    theme(axis.text.x=element_text(angle = 90, vjust = .5, hjust=1))

res_validation$within_sample = average
```


## Between-sample comparisons
For this, we need to look at every cell type independently.
Works for all methods except CIBERSORT.

```{r, fig.width=12, fig.height=12, echo=FALSE, fig.cap="Correlations of known vs. predicted fractions for each cell type independenctly. The 'Tcell' column corresponds to the amount of profiled total T cells or the sum of CD4+ and CD8+ T cells respectively. "}
all_results_ref %>%
  # filter(!(cell_type == "T cell" & dataset != "hoek")) %>%
  ggplot(aes(x=true_fraction, y=estimate)) +
    geom_point(aes(color=cell_type, shape=dataset)) +
    scale_color_manual(values=color_scales$validation) +
    facet_grid(method~cell_type, scales = "free_y") +
    geom_abline(slope = 1, intercept = 0, color="grey") + 
    theme_bw() +
    theme(legend.position = "top",
          strip.text.x = element_text(size=9)) +
    background_grid(major="xy") +
    stat_cor(method='pearson')
```

Compute the average over all cell types.
We only use cell types with at least 5 samples to obtain reasonable
correlation estimates. We also exclude the T-cell supercategory to
avoid redundancies.

```{r}
use_cell_types2 = c("B cell", "Dendritic cell", "Macrophage/Monocyte", "NK cell", "T cell CD4+", "T cell CD8+")
```

```{r, fig.width=5, fig.height=4, echo=FALSE, fig.cap="Performance on validation datasets by cell type. The last column shows the mean over all cell types. "}
cell_type_correlations = all_results_ref %>%
#   filter(method %in% between_methods) %>%
  filter(cell_type %in% use_cell_types2) %>%
  group_by(cell_type, method) %>%
  do(make_cor(.$true_fraction, .$estimate))

average = cell_type_correlations %>%
  group_by(method) %>%
  summarise(pearson = mean(pearson)) %>%
  mutate(cell_type = "mean")

cell_type_correlations %>%
  bind_rows(average) %>%
  ungroup() %>%
  mutate(pearson_text = if_else(pearson < 0, "< 0", as.character(round(pearson, 2))),
         pearson = if_else(pearson < 0, 0, pearson)) %>%
  mutate(cell_type = factor(cell_type, levels = c(sort(use_cell_types2), "mean"))) %>%
  ggplot(aes(x=cell_type, y=method)) +
    geom_tile(aes(fill=pearson)) +
    geom_text(aes(label=pearson_text), size=3) +
    scale_fill_distiller(type="div", palette = "RdYlGn", direction=1, values=c(0,1))  +
    theme(axis.text.x=element_text(angle = 90, vjust = .5, hjust=1))

res_validation$between_sample_average = average
res_validation$between_sample = cell_type_correlations
res_validation$use_cell_types = use_cell_types2
```

Note that, although the correlation for CD8+ T cells looks bad, it does not necessarily mean the predictions are bad. There is just little variance between the samples and CD8+ T cell abundance is generally low, which is correlctly predicted by the methods.




